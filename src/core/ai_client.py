from abc import ABC, abstractmethod
from typing import Generator, Optional, List, Dict
import openai
import httpx
import json
import hashlib
import time
from config import settings
from utils.logger import setup_logger
from utils.privacy import PrivacyFilter

logger = setup_logger(__name__)

class AIClient(ABC):
    """Abstract base class for AI clients."""
    
    @abstractmethod
    def stream_explanation(self, text: str) -> Generator[str, None, None]:
        """
        Streams an explanation for the provided text.
        
        Args:
            text: The selected text to explain.
            
        Yields:
            Chunks of the explanation.
        """
        pass
    
    @abstractmethod
    def generate_follow_up_questions(self, original_text: str, explanation: str) -> List[str]:
        """
        Generate follow-up questions based on the original text and its explanation.
        
        Args:
            original_text: The original selected text.
            explanation: The AI-generated explanation.
            
        Returns:
            A list of follow-up questions.
        """
        pass

class OpenAIClient(AIClient):
    """Enhanced OpenAI API client with timeout, retry, and caching.
    
    Features:
    - Automatic timeout control
    - Retry on transient failures
    - Response caching for identical queries
    - Privacy filtering
    - Input length limiting
    """
    
    def __init__(self):
        self.api_key = settings.OPENAI_API_KEY
        self.base_url = settings.OPENAI_BASE_URL
        self.model = settings.AI_MODEL
        self.timeout = settings.AI_TIMEOUT
        self.max_tokens = settings.AI_MAX_TOKENS
        self.temperature = settings.AI_TEMPERATURE
        
        if not self.api_key:
            logger.warning("OpenAI API Key is missing in configuration.")
        
        # Initialize HTTP client with timeout
        http_client = httpx.Client(
            timeout=httpx.Timeout(
                timeout=self.timeout,
                connect=10.0,  # Connection timeout
                read=self.timeout,  # Read timeout
                write=10.0,  # Write timeout
                pool=5.0  # Pool timeout
            )
        )
        
        # Initialize OpenAI client with custom http_client
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=http_client
        )
        
        # Response cache
        self.cache: Dict[str, str] = {}
        self.cache_enabled = settings.ENABLE_CACHE
        self.cache_max_size = settings.CACHE_MAX_SIZE
        
        logger.info(f"OpenAI Client initialized: model={self.model}, timeout={self.timeout}s")

    def _get_cache_key(self, text: str) -> str:
        """Generate cache key from text."""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _add_to_cache(self, key: str, response: str):
        """Add response to cache with LRU eviction."""
        if not self.cache_enabled:
            return
        
        # LRU eviction: remove oldest entry if cache is full
        if len(self.cache) >= self.cache_max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            logger.debug(f"Cache eviction: removed oldest entry")
        
        self.cache[key] = response
        logger.debug(f"Cached response for key: {key[:8]}...")
    
    def _get_from_cache(self, key: str) -> Optional[str]:
        """Get response from cache."""
        if not self.cache_enabled:
            return None
        return self.cache.get(key)
    
    def stream_explanation(self, text: str) -> Generator[str, None, None]:
        """
        Streams explanation from AI provider with robust error handling.
        
        Features:
        - Privacy filtering
        - Response caching
        - Input length limiting
        - Comprehensive error handling
        - Timeout control
        
        Args:
            text: The text to explain
            
        Yields:
            Chunks of the explanation
        """
        # 1. Privacy Filter
        if settings.ENABLE_PRIVACY_FILTER and PrivacyFilter.contains_sensitive_data(text):
            logger.warning("âš ï¸ Sensitive data detected. Aborting AI request.")
            yield "âš ï¸ **æ£€æµ‹åˆ°æ•æ„Ÿä¿¡æ¯**\n\n"
            yield "ä¸ºä¿æŠ¤æ‚¨çš„éšç§ï¼Œæ­¤è¯·æ±‚å·²è¢«æ‹¦æˆªã€‚\n\n"
            yield "å¯èƒ½åŒ…å«ï¼šä¿¡ç”¨å¡å·ã€å¯†ç ã€API Key ç­‰æ•æ„Ÿæ•°æ®ã€‚"
            return

        # 2. Validate API Key
        if not self.api_key:
            logger.error("API Key not configured")
            yield "âŒ **é…ç½®é”™è¯¯**\n\næœªé…ç½® API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚"
            return

        # 3. Input Length Limiting (prevent excessive costs)
        original_length = len(text)
        if original_length > 5000:
            text = text[:5000]
            logger.warning(f"Input text truncated: {original_length} -> 5000 chars")
            yield "âš ï¸ *è¾“å…¥æ–‡æœ¬è¿‡é•¿ï¼Œå·²è‡ªåŠ¨æˆªæ–­è‡³ 5000 å­—ç¬¦*\n\n"

        # 4. Check Cache
        cache_key = self._get_cache_key(text)
        cached_response = self._get_from_cache(cache_key)
        
        if cached_response:
            logger.info("âœ… Using cached response")
            # Simulate streaming for cached response
            for i in range(0, len(cached_response), 20):
                yield cached_response[i:i+20]
                time.sleep(0.01)  # Smooth display
            return

        # 5. Prepare System Prompt
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è§£é‡Šç”¨æˆ·æä¾›çš„æ–‡æœ¬ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. å¿…é¡»ä½¿ç”¨**ä¸­æ–‡**è¿›è¡Œè§£é‡Šã€‚\n"
            "2. è§£é‡Šè¦ç®€æ´æ˜äº†ï¼Œç›´æ¥æŒ‡å‡ºæ ¸å¿ƒå«ä¹‰ã€‚\n"
            "3. å¦‚æœæ˜¯ä¸“æœ‰åè¯ï¼Œå…ˆç»™å‡ºä¸­æ–‡ç¿»è¯‘ï¼Œå†è§£é‡Šå…¶ç”¨é€”ã€‚\n"
            "4. ä½¿ç”¨ Markdown æ ¼å¼ï¼Œé‡ç‚¹å†…å®¹å¯ä»¥ä½¿ç”¨ç²—ä½“ã€‚\n"
            "5. æ§åˆ¶å­—æ•°åœ¨300å­—ä»¥å†…ï¼Œä¸è¦è¾“å‡ºé•¿ç¯‡å¤§è®ºï¼Œåªè§£é‡Šæœ€æ ¸å¿ƒçš„æ¦‚å¿µã€‚"
        )
        
        # 6. Make API Request with Error Handling
        full_response = ""
        try:
            logger.info(f"Making API request: model={self.model}, input_length={len(text)}")
            
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                stream=True,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
            
            # Cache the complete response
            if full_response:
                self._add_to_cache(cache_key, full_response)
                logger.info(f"âœ… API request completed: {len(full_response)} chars")

        except httpx.TimeoutException as e:
            logger.error(f"â±ï¸ Request timeout: {e}")
            yield "\n\nâŒ **è¯·æ±‚è¶…æ—¶**\n\n"
            yield f"æœåŠ¡å™¨å“åº”æ—¶é—´è¶…è¿‡ {self.timeout} ç§’ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
        
        except openai.APIConnectionError as e:
            logger.error(f"ğŸ”Œ API Connection Error: {e}")
            yield "\n\nâŒ **æ— æ³•è¿æ¥åˆ° AI æœåŠ¡**\n\n"
            yield "è¯·æ£€æŸ¥ï¼š\n"
            yield "1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
            yield "2. API Base URL æ˜¯å¦æ­£ç¡®\n"
            yield f"3. å½“å‰é…ç½®: {self.base_url}"
        
        except openai.AuthenticationError as e:
            logger.error(f"ğŸ”‘ API Auth Error: {e}")
            yield "\n\nâŒ **è®¤è¯å¤±è´¥**\n\n"
            yield "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ OPENAI_API_KEYã€‚"
        
        except openai.RateLimitError as e:
            logger.error(f"âš ï¸ Rate Limit Error: {e}")
            yield "\n\nâŒ **è¯·æ±‚é¢‘ç‡è¶…é™**\n\n"
            yield "API è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        except openai.BadRequestError as e:
            logger.error(f"âŒ Bad Request Error: {e}")
            yield "\n\nâŒ **è¯·æ±‚å‚æ•°é”™è¯¯**\n\n"
            yield f"è¯·æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚å½“å‰æ¨¡å‹: {self.model}"
        
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error in AI request: {e}", exc_info=True)
            yield f"\n\nâŒ **å‘ç”ŸæœªçŸ¥é”™è¯¯**\n\n{str(e)}"
    
    def generate_follow_up_questions(self, original_text: str, explanation: str) -> List[str]:
        """
        ç”Ÿæˆç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„åç»­é—®é¢˜ï¼ˆæ‰©å±•æŸ¥è¯¢æ‰‹ï¼‰ã€‚
        
        Args:
            original_text: ç”¨æˆ·é€‰ä¸­çš„åŸå§‹æ–‡æœ¬
            explanation: AI ç”Ÿæˆçš„è§£é‡Šå†…å®¹
            
        Returns:
            åŒ…å« 3-5 ä¸ªåç»­é—®é¢˜çš„åˆ—è¡¨
        """
        if not self.api_key:
            logger.warning("API Key is not configured for follow-up questions.")
            return []
        
        # Limit input length to control costs
        if len(original_text) > 500:
            original_text = original_text[:500] + "..."
        if len(explanation) > 1000:
            explanation = explanation[:1000] + "..."
        
        try:
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®é¢˜æ¨èåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬å’Œå·²ç”Ÿæˆçš„è§£é‡Šï¼Œ"
                "æ¨ç†å‡ºç”¨æˆ·å¯èƒ½ä¼šè¿›ä¸€æ­¥æ„Ÿå…´è¶£çš„é—®é¢˜ã€‚\n\n"
                "è¦æ±‚ï¼š\n"
                "1. ç”Ÿæˆ 3-5 ä¸ªç›¸å…³çš„åç»­é—®é¢˜\n"
                "2. é—®é¢˜è¦å…·ä½“ã€æœ‰é’ˆå¯¹æ€§ï¼Œä¸è¦å¤ªæ³›æ³›\n"
                "3. é—®é¢˜åº”è¯¥ç”±æµ…å…¥æ·±ï¼Œæ¶µç›–ä¸åŒè§’åº¦ï¼ˆå¦‚å†å²ã€åº”ç”¨ã€åŸç†ã€å¯¹æ¯”ç­‰ï¼‰\n"
                "4. æ¯ä¸ªé—®é¢˜æ§åˆ¶åœ¨ 15 å­—ä»¥å†…\n"
                "5. ç›´æ¥è¿”å› JSON æ•°ç»„æ ¼å¼ï¼Œä¾‹å¦‚ï¼š[\"é—®é¢˜1\", \"é—®é¢˜2\", \"é—®é¢˜3\"]\n"
                "6. ä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ï¼Œåªè¿”å› JSON æ•°ç»„"
            )
            
            user_prompt = (
                f"ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ï¼š{original_text}\n\n"
                f"å·²ç”Ÿæˆçš„è§£é‡Šï¼š\n{explanation}\n\n"
                f"è¯·ç”Ÿæˆ 3-5 ä¸ªç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„åç»­é—®é¢˜ï¼š"
            )
            
            logger.debug("Generating follow-up questions...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=200,  # Limit token usage
                stream=False
            )
            
            content = response.choices[0].message.content.strip()
            logger.debug(f"Follow-up questions raw response: {content[:100]}...")
            
            # Parse JSON
            try:
                # Remove markdown code block markers
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                questions = json.loads(content)
                
                if isinstance(questions, list):
                    # Filter and clean questions
                    questions = [q.strip() for q in questions if isinstance(q, str) and q.strip()]
                    # Limit to 5 questions
                    questions = questions[:5]
                    logger.info(f"âœ… Generated {len(questions)} follow-up questions")
                    return questions
                else:
                    logger.warning(f"Invalid JSON format: expected list, got {type(questions)}")
                    return []
                    
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse follow-up questions JSON: {e}")
                logger.error(f"Content was: {content}")
                return []
        
        except httpx.TimeoutException as e:
            logger.error(f"Timeout generating follow-up questions: {e}")
            return []
        
        except openai.APIConnectionError as e:
            logger.error(f"API Connection Error in follow-up questions: {e}")
            return []
        
        except openai.AuthenticationError as e:
            logger.error(f"API Auth Error in follow-up questions: {e}")
            return []
        
        except Exception as e:
            logger.error(f"Unexpected error in follow-up questions: {e}", exc_info=True)
            return []
